{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79dc2ee3-82e9-44fe-9c85-e0f49b8a35e8",
   "metadata": {},
   "source": [
    "# Issue 132 Notebook Toaletter n√§ra SAT\n",
    "* denna [Notebook](https://github.com/salgo60/Stockholm_Archipelago_Trail/blob/main/notebook/Issue_132_Notebook_Toaletter_n%C3%A4ra_SAT.ipynb)\n",
    "* [Issue 132](https://github.com/salgo60/Stockholm_Archipelago_Trail/issues/132)\n",
    "\n",
    "Se liknande l√∂sning f√∂r Roslagsleden\n",
    "* nu har vi SAT = wikidata [Q131318799](https://www.wikidata.org/wiki/Q131318799)\n",
    "* \"leden\" sitter inte ihop utan varje √∂ har sitt segment\n",
    "\n",
    "Jmf dricksvatten [Issue 139](https://github.com/salgo60/Stockholm_Archipelago_Trail/issues/139) \n",
    "\n",
    "\n",
    "Output \n",
    "* [kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_07_28_17_12.html](https://raw.githack.com/salgo60/Stockholm_Archipelago_Trail/main/kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_07_28_17_12.html)\n",
    "* [kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_07_30_04_22.html](https://raw.githack.com/salgo60/Stockholm_Archipelago_Trail/main/kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_07_30_04_22.html)\n",
    "* [kartor/Issue_132_2_toaletter_nara_stockholm_archipelago_trail_2025_07_30_07_16.html](https://raw.githack.com/salgo60/Stockholm_Archipelago_Trail/main/kartor/Issue_132_2_toaletter_nara_stockholm_archipelago_trail_2025_07_30_07_16.html)\n",
    "* [kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_17_19_29.html](https://raw.githack.com/salgo60/Stockholm_Archipelago_Trail/main/kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_17_19_29.html)\n",
    "* [kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_19_16_01.html](https://raw.githack.com/salgo60/Stockholm_Archipelago_Trail/main/kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_19_16_01.html)\n",
    "* [kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_19_23_14.html](https://raw.githack.com/salgo60/Stockholm_Archipelago_Trail/main/kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_19_23_14.html)\n",
    "* [kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_26_15_55.html](https://raw.githack.com/salgo60/Stockholm_Archipelago_Trail/main/kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_26_15_55.html)\n",
    "* [kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_27_16_53.html](https://raw.githack.com/salgo60/Stockholm_Archipelago_Trail/main/kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_27_16_53.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd494e0-4277-42cd-8f35-d97c83229c07",
   "metadata": {},
   "source": [
    "version 0.1\n",
    "* unary_union --> union_all\n",
    "* start updating OSM with pictures from https://commons.wikimedia.org/wiki/Category:SAT_Todo\n",
    "* also count number of seats ie. toilets:number\n",
    "* Fetch SAT etapper via Wikidata\n",
    "* Fetch geometries per relation via Overpass (no zip-mismatch)\n",
    "* Buffer 200 m (Shapely ‚â•2 with union_all())\n",
    "* Fetch toilets as nodes/ways/relations via nwr[...] out center;\n",
    "* Count toilets via toilets:number (fallback male/female/unisex ‚Üí else 1)\n",
    "* Join to nearest etapp, build summary (CSV)\n",
    "* Build a Folium map with etapper, 200 m buffer, and toilet markers\n",
    "* Export toilets within 200 m as GeoJSON + CSV and save the map HTML\n",
    "\n",
    "version 0.2\n",
    "* added mapillary\n",
    "* added toilets:paper_supplied\n",
    "* getting error fetching OSM with overpass moved it to one call instead one per segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98b8e5d-53c1-4e11-b150-d75fdeabad97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2025-08-27 16:53:47\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "timestamp = now.timestamp()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Start:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0bc4445-e977-4760-8545-44c54a827dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç H√§mtar SAT-etapper fr√•n Wikidata...\n",
      "‚úÖ Hittade 20 etapper med OSM-relationer\n",
      "üåê Overpass endpoint: https://overpass.kumi.systems/api/interpreter\n",
      "üì° H√§mtar geometrier fr√•n Overpass (inkl. n√§stlade relationer) ...\n",
      "\n",
      "--- OVERPASS QUERY (batch) ---\n",
      "\n",
      "[out:json][timeout:180];\n",
      "rel(id:19012436,19020231,19013576,19079703,19013473,19014571,19015969,19012684,19012654,19023687,19016280,19014515)->.R;\n",
      ".R out body;\n",
      "(.R; >>;)->.ALL;\n",
      "way.ALL;\n",
      "out geom tags;\n",
      "\n",
      "--- END QUERY ---\n",
      "\n",
      "\n",
      "--- OVERPASS QUERY (batch) ---\n",
      "\n",
      "[out:json][timeout:180];\n",
      "rel(id:19141225,19013472,19080874,19018272,19020310,19023630,19016187,19081125)->.R;\n",
      ".R out body;\n",
      "(.R; >>;)->.ALL;\n",
      "way.ALL;\n",
      "out geom tags;\n",
      "\n",
      "--- END QUERY ---\n",
      "\n",
      "‚úÖ Klart. Relationer: 20 | Linjesegment totalt: 625\n",
      "üß© Etapper med geometri: 20 / 20 (saknade/ogiltiga: 0)\n",
      "üßÆ Skapar 200 m-buffert och 200‚Äì400 m-ring...\n",
      "\n",
      "--- OVERPASS QUERY (toilets) ---\n",
      "\n",
      "[out:json][timeout:60];\n",
      "nwr[\"amenity\"=\"toilets\"](58.7378793,17.8574954,59.8625492,19.1391668);\n",
      "out center;\n",
      "\n",
      "--- END QUERY ---\n",
      "\n",
      "‚úÖ Hittade 726 toalett-objekt inom bbox\n",
      "‚úÖ 101 toalett-objekt inom/vid 200 m\n",
      "‚úÖ 9 toalett-objekt inom 200‚Äì400 m\n",
      "üìä Summary (‚â§200 m):\n",
      "            label     island  sites  toilets_total  avg_distance_m  \\\n",
      "4      SAT Grinda     Grinda     17             35       33.291670   \n",
      "10    SAT N√•ttar√∂    N√•ttar√∂     11             28       72.232629   \n",
      "1    SAT Finnhamn   Finnhamn     16             24       29.151231   \n",
      "2   SAT Fj√§rdl√•ng  Fj√§rdl√•ng     11             18       59.117606   \n",
      "0     SAT Arholma    Arholma      8             13       41.423235   \n",
      "14        SAT Ut√∂        Ut√∂      7             11       17.180006   \n",
      "12       SAT R√•n√∂       R√•n√∂      6             11       32.477367   \n",
      "5    SAT Ingmars√∂   Ingmars√∂      5              8       24.822614   \n",
      "13   SAT Sandhamn     Sand√∂n      4              5       16.953942   \n",
      "7        SAT Lid√∂       Lid√∂      3              4       47.205404   \n",
      "16        SAT √Öl√∂        √Öl√∂      3              4       35.464055   \n",
      "8        SAT M√∂ja       M√∂ja      3              3       17.928814   \n",
      "6    SAT Landsort        √ñja      2              3       16.946321   \n",
      "9       SAT N√§md√∂      N√§md√∂      2              2        8.259667   \n",
      "3    SAT Furusund   Furusund      1              1       16.958877   \n",
      "11       SAT Orn√∂       Orn√∂      1              1        4.276034   \n",
      "15      SAT Yxlan      Yxlan      1              1       94.323707   \n",
      "\n",
      "    avg_toilets_per_site  \n",
      "4               2.058824  \n",
      "10              2.545455  \n",
      "1               1.500000  \n",
      "2               1.636364  \n",
      "0               1.625000  \n",
      "14              1.571429  \n",
      "12              1.833333  \n",
      "5               1.600000  \n",
      "13              1.250000  \n",
      "7               1.333333  \n",
      "16              1.333333  \n",
      "8               1.000000  \n",
      "6               1.500000  \n",
      "9               1.000000  \n",
      "3               1.000000  \n",
      "11              1.000000  \n",
      "15              1.000000  \n",
      "üß≠ Map center: [59.27498398984334, 18.608217673582555]\n",
      "üßæ Basic CSV (‚â§200 m): ../kartor/sat_toaletter_basic_tags_0_200_2025_08_27_16_53.csv\n",
      "üßæ Basic CSV (200‚Äì400 m): ../kartor/sat_toaletter_basic_tags_200_400_2025_08_27_16_53.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/md6r13sj0wsbg_6_xl160d300000gn/T/ipykernel_8471/3321270200.py:366: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  trail_union_utm = meta_gdf.to_crs(3006).geometry.unary_union\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Klart!\n",
      "‚Ä¢ Summary CSV (‚â§200 m): ../kartor/sat_toaletter_summary_2025_08_27_16_53.csv\n",
      "‚Ä¢ Toilets GeoJSON ‚â§200 m: ../kartor/sat_toaletter_inrange_0_200_2025_08_27_16_53.geojson\n",
      "‚Ä¢ Toilets CSV ‚â§200 m: ../kartor/sat_toaletter_inrange_0_200_2025_08_27_16_53.csv\n",
      "‚Ä¢ Toilets GeoJSON 200‚Äì400 m: ../kartor/sat_toaletter_inrange_200_400_2025_08_27_16_53.geojson\n",
      "‚Ä¢ Toilets CSV 200‚Äì400 m: ../kartor/sat_toaletter_inrange_200_400_2025_08_27_16_53.csv\n",
      "‚Ä¢ Basic CSV ‚â§200 m: ../kartor/sat_toaletter_basic_tags_0_200_2025_08_27_16_53.csv\n",
      "‚Ä¢ Basic CSV 200‚Äì400 m: ../kartor/sat_toaletter_basic_tags_200_400_2025_08_27_16_53.csv\n",
      "‚Ä¢ Karta: ../kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_27_16_53.html\n"
     ]
    }
   ],
   "source": [
    "# !pip install geopandas shapely folium requests SPARQLWrapper --quiet\n",
    "\n",
    "import os, re, requests, html, time, random, json \n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import LineString, MultiLineString, Point, mapping\n",
    "from shapely.ops import linemerge, unary_union\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import folium\n",
    "from folium import Marker, Icon, FeatureGroup, LayerControl, Popup\n",
    "from collections import defaultdict, deque\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================\n",
    "# 1) H√§mta SAT-etapper via Wikidata\n",
    "# =========================\n",
    "print(\"üîç H√§mtar SAT-etapper fr√•n Wikidata...\")\n",
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "sparql.setQuery(\"\"\"\n",
    "SELECT ?item ?itemLabel ?islandLabel ?osmid WHERE {\n",
    "  ?item wdt:P361 wd:Q131318799;\n",
    "        wdt:P31 wd:Q2143825;\n",
    "        wdt:P402 ?osmid.\n",
    "  OPTIONAL { ?item wdt:P706 ?island. }\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"sv,en\". }\n",
    "}\n",
    "\"\"\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "etapper = []\n",
    "for r in results[\"results\"][\"bindings\"]:\n",
    "    etapper.append({\n",
    "        \"id\": r[\"osmid\"][\"value\"],\n",
    "        \"label\": r.get(\"itemLabel\", {}).get(\"value\", \"\"),\n",
    "        \"island\": r.get(\"islandLabel\", {}).get(\"value\", \"\")\n",
    "    })\n",
    "osm_ids = [e['id'] for e in etapper]\n",
    "print(f\"‚úÖ Hittade {len(osm_ids)} etapper med OSM-relationer\")\n",
    "\n",
    "# =========================\n",
    "# 2) Overpass helpers\n",
    "# =========================\n",
    "OVERPASS_ENDPOINTS = [\n",
    "    \"https://overpass.kumi.systems/api/interpreter\",\n",
    "    \"https://overpass-api.de/api/interpreter\",\n",
    "]\n",
    "\n",
    "def overpass_data_age_info(data):\n",
    "    try:\n",
    "        ts = data.get(\"osm3s\", {}).get(\"timestamp_osm_base\")\n",
    "        if ts:\n",
    "            print(f\"üïí Overpass-databasens timestamp: {ts}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def _sleep_backoff(i):\n",
    "    wait = (2 ** i) + random.random()\n",
    "    time.sleep(wait)\n",
    "\n",
    "def _overpass_post_json(query, endpoints=None, retries=3, timeout=120):\n",
    "    \"\"\"\n",
    "    Try endpoints in order; return (data_dict, endpoint_used, error_msg).\n",
    "    \"\"\"\n",
    "    endpoints = endpoints or OVERPASS_ENDPOINTS\n",
    "    last_err = None\n",
    "    for ep in endpoints:\n",
    "        for i in range(retries):\n",
    "            try:\n",
    "                r = requests.post(ep, data={\"data\": query}, timeout=timeout)\n",
    "                if r.status_code == 200 and \"json\" in (r.headers.get(\"Content-Type\",\"\").lower()):\n",
    "                    try:\n",
    "                        return r.json(), ep, None\n",
    "                    except Exception as e:\n",
    "                        last_err = f\"JSON decode error: {e}\"\n",
    "                else:\n",
    "                    head = (r.text or \"\")[:300].replace(\"\\n\", \" \")\n",
    "                    last_err = f\"Unexpected response {r.status_code} {r.headers.get('Content-Type')} ‚Äî {head}\"\n",
    "            except Exception as e:\n",
    "                last_err = f\"{type(e).__name__}: {e}\"\n",
    "            if i < retries - 1:\n",
    "                _sleep_backoff(i)\n",
    "        # try next endpoint\n",
    "    return None, None, last_err\n",
    "\n",
    "def _pick_endpoint():\n",
    "    test_q = \"[out:json][timeout:25];node(0,0,0,0);out;\"\n",
    "    data, ep, err = _overpass_post_json(test_q, retries=1, timeout=25)\n",
    "    return ep or OVERPASS_ENDPOINTS[0]\n",
    "\n",
    "overpass_url = _pick_endpoint()\n",
    "print(f\"üåê Overpass endpoint: {overpass_url}\")\n",
    "\n",
    "def _chunks(iterable, n):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        batch = []\n",
    "        try:\n",
    "            for _ in range(n):\n",
    "                batch.append(next(it))\n",
    "        except StopIteration:\n",
    "            pass\n",
    "        if not batch:\n",
    "            break\n",
    "        yield batch\n",
    "\n",
    "# =========================\n",
    "# 2a) H√§mta geometrier per relation (rekursivt)\n",
    "# =========================\n",
    "print(\"üì° H√§mtar geometrier fr√•n Overpass (inkl. n√§stlade relationer) ...\")\n",
    "\n",
    "geom_per_rel = {}   # {rel_id: [LineString, ...]}\n",
    "geom_per_rel_meta = defaultdict(lambda: {\"mapillary_urls\": set()})\n",
    "all_lines = []\n",
    "\n",
    "# Pre-init relations\n",
    "for rid in osm_ids:\n",
    "    geom_per_rel.setdefault(int(rid), [])\n",
    "\n",
    "CHUNK_SIZE = 12  # minska till 6 eller 4 om timeouts\n",
    "\n",
    "def _mapillary_urls_from_tags(tags):\n",
    "    raw = (tags or {}).get(\"mapillary\", \"\")\n",
    "    if not raw or not str(raw).strip():\n",
    "        return []\n",
    "    keys = [k for k in re.split(r\"[;,\\s]+\", str(raw).strip()) if k]\n",
    "    return [f\"https://www.mapillary.com/app/?pKey={k}\" for k in keys]\n",
    "\n",
    "for batch_ids in _chunks([int(x) for x in osm_ids], CHUNK_SIZE):\n",
    "    ids_str = \",\".join(map(str, batch_ids))\n",
    "\n",
    "    # ‚ö†Ô∏è IMPORTANT: no comments; split 'way.ALL; out geom tags;'\n",
    "    q = f\"\"\"\n",
    "[out:json][timeout:180];\n",
    "rel(id:{ids_str})->.R;\n",
    ".R out body;\n",
    "(.R; >>;)->.ALL;\n",
    "way.ALL;\n",
    "out geom tags;\n",
    "\"\"\".strip()\n",
    "\n",
    "    # (Optional) print query so you can run it in Overpass Turbo / curl\n",
    "    print(\"\\n--- OVERPASS QUERY (batch) ---\\n\")\n",
    "    print(q)\n",
    "    print(\"\\n--- END QUERY ---\\n\")\n",
    "\n",
    "    data, used_ep, err = _overpass_post_json(q, endpoints=[overpass_url] + [e for e in OVERPASS_ENDPOINTS if e != overpass_url], retries=3, timeout=180)\n",
    "    if not isinstance(data, dict):\n",
    "        print(f\"‚ùå Misslyckades att h√§mta batch: {batch_ids} ‚Äî {err}\")\n",
    "        continue\n",
    "    if used_ep and used_ep != overpass_url:\n",
    "        overpass_url = used_ep  # switch for subsequent calls\n",
    "\n",
    "    elements = data.get(\"elements\", []) or []\n",
    "    rel_elems = [e for e in elements if e.get(\"type\") == \"relation\"]\n",
    "    way_elems = [e for e in elements if e.get(\"type\") == \"way\"]\n",
    "\n",
    "    rel_children = defaultdict(set)\n",
    "    rel_direct_ways = defaultdict(set)\n",
    "\n",
    "    for rel in rel_elems:\n",
    "        rid = rel.get(\"id\")\n",
    "        for m in rel.get(\"members\", []) or []:\n",
    "            if m.get(\"type\") == \"relation\":\n",
    "                rel_children[rid].add(m.get(\"ref\"))\n",
    "            elif m.get(\"type\") == \"way\":\n",
    "                rel_direct_ways[rid].add(m.get(\"ref\"))\n",
    "\n",
    "    way_geom = {}\n",
    "    way_mapillary = defaultdict(list)\n",
    "    for w in way_elems:\n",
    "        if \"geometry\" not in w:\n",
    "            continue\n",
    "        coords = [(pt[\"lon\"], pt[\"lat\"]) for pt in w[\"geometry\"] if \"lon\" in pt and \"lat\" in pt]\n",
    "        if len(coords) < 2:\n",
    "            continue\n",
    "        try:\n",
    "            line = LineString(coords)\n",
    "        except Exception:\n",
    "            continue\n",
    "        way_geom[w[\"id\"]] = line\n",
    "        for url in _mapillary_urls_from_tags(w.get(\"tags\", {})):\n",
    "            way_mapillary[w[\"id\"]].append(url)\n",
    "\n",
    "    # Collect ALL descendant ways via BFS\n",
    "    for top_rel in batch_ids:\n",
    "        top_rel = int(top_rel)\n",
    "        visited = set()\n",
    "        stack = deque([top_rel])\n",
    "        all_way_ids = set()\n",
    "\n",
    "        while stack:\n",
    "            current = stack.popleft()\n",
    "            if current in visited:\n",
    "                continue\n",
    "            visited.add(current)\n",
    "            all_way_ids |= rel_direct_ways.get(current, set())\n",
    "            for child in rel_children.get(current, set()):\n",
    "                if child not in visited:\n",
    "                    stack.append(child)\n",
    "\n",
    "        rel_lines = []\n",
    "        for wid in all_way_ids:\n",
    "            geom = way_geom.get(wid)\n",
    "            if geom is not None:\n",
    "                rel_lines.append(geom)\n",
    "                all_lines.append(geom)\n",
    "                for u in way_mapillary.get(wid, []):\n",
    "                    geom_per_rel_meta[top_rel][\"mapillary_urls\"].add(u)\n",
    "\n",
    "        if rel_lines:\n",
    "            geom_per_rel[top_rel].extend(rel_lines)\n",
    "\n",
    "    time.sleep(1.0)  # be gentle\n",
    "\n",
    "antal_rel = len(geom_per_rel)\n",
    "antal_linjer = len(all_lines)\n",
    "print(f\"‚úÖ Klart. Relationer: {antal_rel} | Linjesegment totalt: {antal_linjer}\")\n",
    "if antal_linjer == 0:\n",
    "    print(\"‚ö†Ô∏è Overpass svarade utan way-geometrier. S√§nk CHUNK_SIZE (t.ex. 6) eller k√∂r igen senare.\")\n",
    "\n",
    "# =========================\n",
    "# 2b) Bygg etapp-GeoDataFrame + samlad trail\n",
    "# =========================\n",
    "def _normalize_lines(lines):\n",
    "    if not lines:\n",
    "        return None\n",
    "    try:\n",
    "        merged = linemerge(unary_union(lines))\n",
    "    except Exception:\n",
    "        try:\n",
    "            merged = linemerge(MultiLineString(lines))\n",
    "        except Exception:\n",
    "            return None\n",
    "    gtype = getattr(merged, \"geom_type\", \"\")\n",
    "    if gtype in (\"LineString\", \"MultiLineString\"):\n",
    "        return merged\n",
    "    if gtype == \"GeometryCollection\":\n",
    "        only_lines = [g for g in merged.geoms if g.geom_type == \"LineString\"]\n",
    "        if not only_lines:\n",
    "            return None\n",
    "        return only_lines[0] if len(only_lines) == 1 else MultiLineString(only_lines)\n",
    "    return None\n",
    "\n",
    "meta_rows = []\n",
    "count_no_lines = 0\n",
    "\n",
    "for et in etapper:\n",
    "    rel_id = int(et[\"id\"])\n",
    "    lines = geom_per_rel.get(rel_id, []) or []\n",
    "    if not lines:\n",
    "        count_no_lines += 1\n",
    "        continue\n",
    "    norm = _normalize_lines(lines)\n",
    "    if norm is None:\n",
    "        count_no_lines += 1\n",
    "        continue\n",
    "    meta_rows.append({\n",
    "        \"rel_id\": rel_id,\n",
    "        \"label\": et.get(\"label\", \"\"),\n",
    "        \"island\": et.get(\"island\", \"\"),\n",
    "        \"geometry\": norm\n",
    "    })\n",
    "\n",
    "if meta_rows:\n",
    "    meta_gdf = gpd.GeoDataFrame(meta_rows, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "else:\n",
    "    meta_gdf = gpd.GeoDataFrame(\n",
    "        {\"rel_id\": pd.Series(dtype=\"int64\"),\n",
    "         \"label\": pd.Series(dtype=\"object\"),\n",
    "         \"island\": pd.Series(dtype=\"object\")},\n",
    "        geometry=gpd.GeoSeries([], dtype=\"geometry\", crs=\"EPSG:4326\"),\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "\n",
    "gdf_trail = meta_gdf[[\"geometry\"]].copy()\n",
    "print(f\"üß© Etapper med geometri: {len(meta_gdf)} / {len(etapper)} (saknade/ogiltiga: {count_no_lines})\")\n",
    "\n",
    "# =========================\n",
    "# 3) Buffert 200 m + ring 200‚Äì400 m\n",
    "# =========================\n",
    "print(\"üßÆ Skapar 200 m-buffert och 200‚Äì400 m-ring...\")\n",
    "trail_utm = gdf_trail.to_crs(3006)\n",
    "buffer_utm_200 = trail_utm.buffer(200)\n",
    "buffer_utm_400 = trail_utm.buffer(400)\n",
    "\n",
    "buffer_utm_200_u = buffer_utm_200.union_all() if hasattr(buffer_utm_200, \"union_all\") else buffer_utm_200.unary_union\n",
    "buffer_utm_400_u = buffer_utm_400.union_all() if hasattr(buffer_utm_400, \"union_all\") else buffer_utm_400.unary_union\n",
    "ring_utm = buffer_utm_400_u.difference(buffer_utm_200_u)\n",
    "\n",
    "buffer_union = gpd.GeoSeries([buffer_utm_200_u], crs=3006).to_crs(4326).iloc[0]\n",
    "ring_union   = gpd.GeoSeries([ring_utm],       crs=3006).to_crs(4326).iloc[0]\n",
    "\n",
    "# =========================\n",
    "# 4) H√§mta toaletter (Overpass)\n",
    "# =========================\n",
    "def _parse_int(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    m = re.search(r\"\\d+\", str(v))\n",
    "    return int(m.group()) if m else None\n",
    "\n",
    "def toilets_count_from_tags(tags: dict) -> int:\n",
    "    n = _parse_int((tags or {}).get(\"toilets:num_chambers\"))\n",
    "    if n is not None:\n",
    "        return n\n",
    "    n2 = _parse_int((tags or {}).get(\"toilets:number\"))\n",
    "    if n2 is not None:\n",
    "        return n2\n",
    "    parts = [\n",
    "        _parse_int((tags or {}).get(\"male:toilets\")),\n",
    "        _parse_int((tags or {}).get(\"female:toilets\")),\n",
    "        _parse_int((tags or {}).get(\"unisex:toilets\")),\n",
    "    ]\n",
    "    parts = [p for p in parts if p is not None]\n",
    "    return sum(parts) if parts else 1\n",
    "\n",
    "if gdf_trail.empty:\n",
    "    print(\"‚ö†Ô∏è Ingen trail-geometri ‚Äî hoppar √∂ver toalettfr√•gan.\")\n",
    "    gdf_toilets = gpd.GeoDataFrame(columns=[\"geometry\",\"tags\",\"id\",\"osm_type\",\"toilets_num\"], crs=\"EPSG:4326\")\n",
    "else:\n",
    "    bbox = gdf_trail.total_bounds  # [minx, miny, maxx, maxy]\n",
    "    q_toilets = f\"\"\"\n",
    "[out:json][timeout:60];\n",
    "nwr[\"amenity\"=\"toilets\"]({bbox[1]},{bbox[0]},{bbox[3]},{bbox[2]});\n",
    "out center;\n",
    "\"\"\".strip()\n",
    "\n",
    "    print(\"\\n--- OVERPASS QUERY (toilets) ---\\n\")\n",
    "    print(q_toilets)\n",
    "    print(\"\\n--- END QUERY ---\\n\")\n",
    "\n",
    "    data, _, err = _overpass_post_json(q_toilets, retries=4, timeout=120)\n",
    "    if err:\n",
    "        print(f\"‚ùå Overpass (toilets) failed: {err}\")\n",
    "        elements = []\n",
    "    else:\n",
    "        elements = data.get(\"elements\", []) if isinstance(data, dict) else []\n",
    "\n",
    "    toilets = []\n",
    "    for el in elements:\n",
    "        tags = el.get(\"tags\", {}) or {}\n",
    "        typ = el.get(\"type\")\n",
    "        if typ == \"node\":\n",
    "            lon, lat = el.get(\"lon\"), el.get(\"lat\")\n",
    "        else:\n",
    "            center = el.get(\"center\") or {}\n",
    "            lon, lat = center.get(\"lon\"), center.get(\"lat\")\n",
    "        if lon is None or lat is None:\n",
    "            continue\n",
    "        toilets.append({\n",
    "            \"geometry\": Point(lon, lat),\n",
    "            \"tags\": tags,\n",
    "            \"id\": el.get(\"id\"),\n",
    "            \"osm_type\": typ,\n",
    "            \"toilets_num\": toilets_count_from_tags(tags),\n",
    "        })\n",
    "\n",
    "    gdf_toilets = gpd.GeoDataFrame(toilets, crs=\"EPSG:4326\")\n",
    "    print(f\"‚úÖ Hittade {len(gdf_toilets)} toalett-objekt inom bbox\")\n",
    "\n",
    "# =========================\n",
    "# 5) Dela upp med avst√•nd (robust)\n",
    "# =========================\n",
    "# Union av hela leden i meter-CRS\n",
    "trail_union_utm = meta_gdf.to_crs(3006).geometry.unary_union\n",
    "\n",
    "# Ber√§kna avst√•nd i meter f√∂r alla toaletter\n",
    "toilets_utm_all = gdf_toilets.to_crs(3006).copy()\n",
    "toilets_utm_all[\"distance_m\"] = toilets_utm_all.geometry.apply(lambda g: g.distance(trail_union_utm))\n",
    "\n",
    "# Dela upp enligt avst√•nd (inkluderar gr√§nsfallet exakt 200/400m)\n",
    "toilets_utm_0_200 = toilets_utm_all[toilets_utm_all[\"distance_m\"] <= 200].copy()\n",
    "toilets_utm_200_400 = toilets_utm_all[(toilets_utm_all[\"distance_m\"] > 200) & (toilets_utm_all[\"distance_m\"] <= 400)].copy()\n",
    "\n",
    "# Tillbaka till WGS84\n",
    "in_0_200 = toilets_utm_0_200.to_crs(4326)\n",
    "in_200_400 = toilets_utm_200_400.to_crs(4326)\n",
    "\n",
    "print(f\"‚úÖ {len(in_0_200)} toalett-objekt inom/vid 200 m\")\n",
    "print(f\"‚úÖ {len(in_200_400)} toalett-objekt inom 200‚Äì400 m\")\n",
    "\n",
    "# =========================\n",
    "# 6) N√§rmaste etapp per kategori\n",
    "# =========================\n",
    "meta_utm = meta_gdf.to_crs(3006)\n",
    "joined_0_200 = gpd.sjoin_nearest(\n",
    "    toilets_utm_0_200,\n",
    "    meta_utm[[\"label\", \"island\", \"geometry\"]],\n",
    "    how=\"left\",\n",
    "    distance_col=\"distance_m\"\n",
    ").to_crs(4326)\n",
    "\n",
    "joined_200_400 = gpd.sjoin_nearest(\n",
    "    toilets_utm_200_400,\n",
    "    meta_utm[[\"label\", \"island\", \"geometry\"]],\n",
    "    how=\"left\",\n",
    "    distance_col=\"distance_m\"\n",
    ").to_crs(4326)\n",
    "\n",
    "joined_0_200 = gpd.sjoin_nearest(\n",
    "    toilets_utm_0_200,\n",
    "    meta_utm[[\"label\", \"island\", \"geometry\"]],\n",
    "    how=\"left\",\n",
    "    distance_col=\"distance_m\"\n",
    ").to_crs(4326)\n",
    "\n",
    "joined_200_400 = gpd.sjoin_nearest(\n",
    "    toilets_utm_200_400,\n",
    "    meta_utm[[\"label\", \"island\", \"geometry\"]],\n",
    "    how=\"left\",\n",
    "    distance_col=\"distance_m\"\n",
    ").to_crs(4326)\n",
    "\n",
    "# =========================\n",
    "# 7) Summary f√∂r ‚â§200 m\n",
    "# =========================\n",
    "summary = (\n",
    "    joined_0_200.assign(toilets_num=joined_0_200[\"toilets_num\"].fillna(1))\n",
    "    .groupby([\"label\", \"island\"], as_index=False)\n",
    "    .agg(\n",
    "        sites=(\"geometry\", \"count\"),\n",
    "        toilets_total=(\"toilets_num\", \"sum\"),\n",
    "        avg_distance_m=(\"distance_m\", \"mean\"),\n",
    "    )\n",
    "    .assign(avg_toilets_per_site=lambda df: df[\"toilets_total\"] / df[\"sites\"])\n",
    "    .sort_values([\"toilets_total\", \"sites\"], ascending=[False, False])\n",
    ")\n",
    "print(\"üìä Summary (‚â§200 m):\")\n",
    "print(summary.head(1000))\n",
    "\n",
    "# -------------------------\n",
    "# Helper: robust map center\n",
    "# -------------------------\n",
    "def compute_map_center(gdf_trail, gdf_toilets=None, default=(59.33, 18.06)):\n",
    "    \"\"\"\n",
    "    Returns [lat, lon] for map center.\n",
    "    Tries trail union centroid -> trail bbox -> toilets bbox -> default.\n",
    "    Safe for empty geometries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if gdf_trail is not None and not gdf_trail.empty:\n",
    "            geom = gdf_trail.geometry\n",
    "            union_geom = geom.union_all() if hasattr(geom, \"union_all\") else geom.unary_union\n",
    "            if union_geom and not union_geom.is_empty:\n",
    "                c = union_geom.centroid\n",
    "                if c and not c.is_empty:\n",
    "                    return [float(c.y), float(c.x)]\n",
    "            minx, miny, maxx, maxy = gdf_trail.total_bounds\n",
    "            if (maxx > minx) and (maxy > miny):\n",
    "                return [float((miny + maxy) / 2.0), float((minx + maxx) / 2.0)]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        if gdf_toilets is not None and not gdf_toilets.empty:\n",
    "            minx, miny, maxx, maxy = gdf_toilets.total_bounds\n",
    "            if (maxx > minx) and (maxy > miny):\n",
    "                return [float((miny + maxy) / 2.0), float((minx + maxx) / 2.0)]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return [float(default[0]), float(default[1])]\n",
    "\n",
    "# =========================\n",
    "# 8) Spara filer + grundkarta\n",
    "# =========================\n",
    "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "os.makedirs(\"../kartor\", exist_ok=True)\n",
    "\n",
    "summary_csv = f\"../kartor/sat_toaletter_summary_{timestamp}.csv\"\n",
    "summary.to_csv(summary_csv, index=False)\n",
    "\n",
    "toilets_geojson_0_200 = f\"../kartor/sat_toaletter_inrange_0_200_{timestamp}.geojson\"\n",
    "toilets_csv_0_200 = f\"../kartor/sat_toaletter_inrange_0_200_{timestamp}.csv\"\n",
    "in_0_200[[\"id\", \"osm_type\", \"toilets_num\", \"tags\", \"geometry\"]].to_file(toilets_geojson_0_200, driver=\"GeoJSON\")\n",
    "in_0_200.drop(columns=\"geometry\").to_csv(toilets_csv_0_200, index=False)\n",
    "\n",
    "toilets_geojson_200_400 = f\"../kartor/sat_toaletter_inrange_200_400_{timestamp}.geojson\"\n",
    "toilets_csv_200_400 = f\"../kartor/sat_toaletter_inrange_200_400_{timestamp}.csv\"\n",
    "in_200_400[[\"id\", \"osm_type\", \"toilets_num\", \"tags\", \"geometry\"]].to_file(toilets_geojson_200_400, driver=\"GeoJSON\")\n",
    "in_200_400.drop(columns=\"geometry\").to_csv(toilets_csv_200_400, index=False)\n",
    "\n",
    "center_latlon = compute_map_center(gdf_trail, gdf_toilets, default=(59.33, 18.06))\n",
    "m = folium.Map(location=center_latlon, zoom_start=9, control_scale=True)\n",
    "print(\"üß≠ Map center:\", center_latlon)\n",
    "\n",
    "# Etapper\n",
    "#colors = [\n",
    "#    \"blue\",\"green\",\"purple\",\"orange\",\"darkred\",\"cadetblue\",\"lightgray\",\"darkblue\",\n",
    "#    \"darkgreen\",\"pink\",\"lightblue\",\"lightgreen\",\"gray\",\"black\",\"beige\",\"lightred\"\n",
    "#] \n",
    "colors = [\n",
    "    \"#0d47a1\", \"#1b5e20\", \"#4a148c\", \"#e65100\",\n",
    "    \"#b71c1c\", \"#006064\", \"#283593\", \"#2e7d32\",\n",
    "    \"#6a1b9a\", \"#01579b\", \"#9e9d24\", \"#4e342e\",\n",
    "    \"#37474f\", \"#7b1fa2\", \"#1565c0\", \"#2e7d32\"\n",
    "]\n",
    "for i, row in meta_gdf.reset_index(drop=True).iterrows():\n",
    "    color = colors[i % len(colors)]\n",
    "    popup = f\"<b>{html.escape(row['label'])}</b><br>√ñ: {html.escape(row['island'])}\"\n",
    "    folium.GeoJson(\n",
    "        data=mapping(row.geometry),\n",
    "        name=row[\"label\"],\n",
    "        style_function=lambda x, c=color: {\"color\": c, \"weight\": 3}\n",
    "    ).add_child(folium.Popup(popup, max_width=360)).add_to(m)\n",
    "\n",
    "# Buffert-lager (200 m och 200‚Äì400 m)\n",
    "folium.GeoJson(\n",
    "    data=mapping(buffer_union),\n",
    "    name=\"200 m Buffert\",\n",
    "    style_function=lambda x: {'fillColor': '#0000ff', 'color': '#0000ff', 'weight': 1, 'fillOpacity': 0.1}\n",
    ").add_to(m)\n",
    "\n",
    "folium.GeoJson(\n",
    "    data=mapping(ring_union),\n",
    "    name=\"Ring 200‚Äì400 m\",\n",
    "    style_function=lambda x: {'fillColor': '#ffa500', 'color': '#ffa500', 'weight': 1, 'fillOpacity': 0.1}\n",
    ").add_to(m)\n",
    "\n",
    "# =========================\n",
    "# 9) Commons / Mapillary helpers + QA\n",
    "# =========================\n",
    "def _quote_commons(title: str) -> str:\n",
    "    return requests.utils.quote(title, safe=':/%')\n",
    "\n",
    "def commons_title_to_page(title: str) -> str:\n",
    "    if not title:\n",
    "        return None\n",
    "    t = title.strip()\n",
    "    low = t.lower()\n",
    "    if low.startswith((\"category:\", \"file:\", \"image:\")):\n",
    "        return f\"https://commons.wikimedia.org/wiki/{_quote_commons(t)}\"\n",
    "    return f\"https://commons.wikimedia.org/wiki/{_quote_commons('File:' + t)}\"\n",
    "\n",
    "def commons_title_to_filepath(title: str, thumb_width=400) -> str | None:\n",
    "    if not title:\n",
    "        return None\n",
    "    t = title.strip()\n",
    "    low = t.lower()\n",
    "    if low.startswith(\"category:\"):\n",
    "        return None\n",
    "    if not low.startswith((\"file:\", \"image:\")):\n",
    "        t = \"File:\" + t\n",
    "    return f\"https://commons.wikimedia.org/wiki/Special:FilePath/{_quote_commons(t)}?width={thumb_width}\"\n",
    "\n",
    "_wikidata_image_cache = {}\n",
    "def wikidata_p18_thumb(qid: str, thumb_width=400) -> str | None:\n",
    "    if not qid:\n",
    "        return None\n",
    "    qid = qid.strip()\n",
    "    if qid in _wikidata_image_cache:\n",
    "        return _wikidata_image_cache[qid]\n",
    "    try:\n",
    "        url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
    "        data = requests.get(url, timeout=15).json()\n",
    "        ent = data.get(\"entities\", {}).get(qid, {})\n",
    "        p18 = ent.get(\"claims\", {}).get(\"P18\", [])\n",
    "        if not p18:\n",
    "            _wikidata_image_cache[qid] = None\n",
    "            return None\n",
    "        filename = p18[0][\"mainsnak\"][\"datavalue\"][\"value\"]\n",
    "        img_url = commons_title_to_filepath(filename, thumb_width=thumb_width)\n",
    "        _wikidata_image_cache[qid] = img_url\n",
    "        return img_url\n",
    "    except Exception:\n",
    "        _wikidata_image_cache[qid] = None\n",
    "        return None\n",
    "\n",
    "def best_image_url_from_tags(tags: dict) -> str | None:\n",
    "    if not tags:\n",
    "        return None\n",
    "    if \"image\" in tags and str(tags[\"image\"]).strip():\n",
    "        first = str(tags[\"image\"]).split(\";\")[0].strip()\n",
    "        if first.lower().startswith((\"http://\", \"https://\")):\n",
    "            return first\n",
    "        return commons_title_to_filepath(first)\n",
    "    if \"wikimedia_commons\" in tags and str(tags[\"wikimedia_commons\"]).strip():\n",
    "        title = str(tags[\"wikimedia_commons\"]).strip()\n",
    "        thumb = commons_title_to_filepath(title)\n",
    "        if thumb:\n",
    "            return thumb\n",
    "    if \"wikidata\" in tags and str(tags[\"wikidata\"]).strip():\n",
    "        return wikidata_p18_thumb(tags[\"wikidata\"])\n",
    "    return None\n",
    "\n",
    "def mapillary_links_from_tags(tags: dict) -> list[str]:\n",
    "    if not tags or \"mapillary\" not in tags or not str(tags[\"mapillary\"]).strip():\n",
    "        return []\n",
    "    raw = str(tags[\"mapillary\"]).strip()\n",
    "    keys = [k for k in re.split(r\"[;,\\s]+\", raw) if k]\n",
    "    return [f\"https://www.mapillary.com/app/?pKey={k}\" for k in keys]\n",
    "\n",
    "BASIC_TAGS = [\n",
    "    \"amenity\", \"access\", \"opening_hours\", \"fee\", \"wheelchair\",\n",
    "    \"toilets:num_chambers\", \"unisex\", \"drinking_water\", \"changing_table\",\n",
    "    \"toilets:paper_supplied\", \"indoor\", \"operator\", \"website\", \"source\"\n",
    "]\n",
    "\n",
    "def missing_tags(tags: dict) -> list:\n",
    "    missing = []\n",
    "    tags = tags or {}\n",
    "    for k in BASIC_TAGS:\n",
    "        if k == \"amenity\":\n",
    "            if tags.get(\"amenity\") != \"toilets\":\n",
    "                missing.append(\"amenity=toilets\")\n",
    "            continue\n",
    "        v = tags.get(k)\n",
    "        if v is None or str(v).strip() == \"\":\n",
    "            missing.append(k)\n",
    "    if \"unisex\" in missing and ((\"male\" in tags or \"male:toilets\" in tags) and (\"female\" in tags or \"female:toilets\" in tags)):\n",
    "        try:\n",
    "            missing.remove(\"unisex\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return missing\n",
    "\n",
    "def build_basic_table(joined_df):\n",
    "    tag_cols = [\n",
    "        \"access\",\"opening_hours\",\"fee\",\"wheelchair\",\"toilets:num_chambers\",\"unisex\",\n",
    "        \"drinking_water\",\"changing_table\",\"toilets:paper_supplied\",\"indoor\",\"operator\",\"website\",\"source\",\n",
    "        \"wikidata\",\"wikimedia_commons\",\"image\",\"mapillary\",\n",
    "        # extra keys you mentioned\n",
    "        \"toilets:handwashing\",\"toilets:position\"\n",
    "    ]\n",
    "    rows = []\n",
    "    for _, r in joined_df.to_crs(4326).iterrows():\n",
    "        tags = r.get(\"tags\", {}) or {}\n",
    "        # Decode if tags arrived as a JSON string\n",
    "        if isinstance(tags, str):\n",
    "            try:\n",
    "                tags = json.loads(tags)\n",
    "            except Exception:\n",
    "                tags = {}\n",
    "\n",
    "        commons_title = str(tags.get(\"wikimedia_commons\") or \"\").strip() or None\n",
    "        commons_page_url = commons_title_to_page(commons_title) if commons_title else None\n",
    "\n",
    "        img_url = best_image_url_from_tags(tags)  # uses image or Commons/Wikidata fallback\n",
    "        miss = missing_tags(tags)\n",
    "        mp_links = mapillary_links_from_tags(tags)\n",
    "\n",
    "        row = {\n",
    "            \"id\": r[\"id\"],\n",
    "            \"osm_type\": r[\"osm_type\"],\n",
    "            \"lat\": r.geometry.y,\n",
    "            \"lon\": r.geometry.x,\n",
    "            \"label\": r.get(\"label\", \"\"),\n",
    "            \"island\": r.get(\"island\", \"\"),\n",
    "            \"distance_m\": round(float(r.get(\"distance_m\", 0)), 1),\n",
    "            \"toilets_num\": int(r.get(\"toilets_num\", 1)),\n",
    "            \"image_url\": img_url,\n",
    "            \"missing_tags\": \", \".join(miss),\n",
    "            \"wikimedia_commons_title\": commons_title,\n",
    "            \"wikimedia_commons_url\": commons_page_url,\n",
    "            \"mapillary_links\": mp_links,\n",
    "        }\n",
    "        for k in tag_cols:\n",
    "            row[k] = (tags.get(k) if isinstance(tags, dict) else None)\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# =========================\n",
    "# 10) Bygg tabeller och lager\n",
    "# =========================\n",
    "basic_df_0_200 = build_basic_table(joined_0_200)\n",
    "basic_df_200_400 = build_basic_table(joined_200_400)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "os.makedirs(\"../kartor\", exist_ok=True)\n",
    "\n",
    "basic_csv_0_200 = f\"../kartor/sat_toaletter_basic_tags_0_200_{timestamp}.csv\"\n",
    "basic_df_0_200.to_csv(basic_csv_0_200, index=False)\n",
    "basic_csv_200_400 = f\"../kartor/sat_toaletter_basic_tags_200_400_{timestamp}.csv\"\n",
    "basic_df_200_400.to_csv(basic_csv_200_400, index=False)\n",
    "\n",
    "print(\"üßæ Basic CSV (‚â§200 m):\", basic_csv_0_200)\n",
    "print(\"üßæ Basic CSV (200‚Äì400 m):\", basic_csv_200_400)\n",
    "\n",
    "def add_markers(df, feature_group, marker_color):\n",
    "    for _, r in df.iterrows():\n",
    "        osm_url = f\"https://www.openstreetmap.org/{'node' if r['osm_type']=='node' else 'way' if r['osm_type']=='way' else 'relation'}/{r['id']}\"\n",
    "        img_html = (f'<div style=\"margin:6px 0\">'f'<img src=\"{html.escape(str(r[\"image_url\"]))}\" loading=\"lazy\" referrerpolicy=\"no-referrer\" '\n",
    "            f'style=\"max-width:280px; height:auto; display:block;\" />'\n",
    "            f'</div>'\n",
    "            ) if r.get(\"image_url\") else \"\"\n",
    "        miss_html = f\"<div style='color:#b00; margin-top:4px'><b>Saknade taggar:</b> {html.escape(str(r['missing_tags']))}</div>\" if r.get(\"missing_tags\") else \"\"\n",
    "        commons_html = \"\"\n",
    "        if r.get(\"wikimedia_commons_url\"):\n",
    "            title = r.get(\"wikimedia_commons_title\") or \"Wikimedia Commons\"\n",
    "            commons_html = f'<div style=\"margin-top:4px\">üì∑ <a href=\"{html.escape(str(r[\"wikimedia_commons_url\"]))}\" target=\"_blank\">{html.escape(str(title))}</a></div>'\n",
    "\n",
    "        mp_links = r.get(\"mapillary_links\") or []\n",
    "        if isinstance(mp_links, str):\n",
    "            mp_links = [u for u in re.split(r\"[;\\s,]+\", mp_links) if u]\n",
    "        mapillary_html = \"\"\n",
    "        if mp_links:\n",
    "            items = \"\".join(f'<li><a href=\"{html.escape(u)}\" target=\"_blank\" rel=\"noopener\">√ñppna i Mapillary</a></li>' for u in mp_links[:5])\n",
    "            mapillary_html = f'<div style=\"margin-top:4px\">üó∫Ô∏è Mapillary:<ul style=\"margin:4px 0 0 18px\">{items}</ul></div>'\n",
    "        kv = []\n",
    "        for k in [\"opening_hours\",\"fee\",\"wheelchair\",\"access\",\"toilets:num_chambers\",\"unisex\",\"drinking_water\",\"changing_table\",\"toilets:paper_supplied\"]:\n",
    "            v = r.get(k)\n",
    "            if pd.notna(v) and str(v).strip() != \"\":\n",
    "                kv.append(f\"{k}={html.escape(str(v))}\")\n",
    "\n",
    "        kv_html = (\"<div>\" + \"<br>\".join(kv) + \"</div>\") if kv else \"\"\n",
    "\n",
    "        popup_html = f\"\"\"\n",
    "        <div style=\"font-size:13px; line-height:1.4\">\n",
    "          <b><a href=\"{osm_url}\" target=\"_blank\">OSM ({r['osm_type']}) {int(r['id'])}</a></b><br>\n",
    "          Etapp: <b>{html.escape(str(r.get('label') or ''))}</b> (√ñ: {html.escape(str(r.get('island') or ''))})<br>\n",
    "          Avst√•nd: ~{r['distance_m']} m<br>\n",
    "          Antal toaletter: <b>{int(r['toilets_num'])}</b>\n",
    "          {img_html}\n",
    "          {kv_html}\n",
    "          {commons_html}\n",
    "          {mapillary_html}\n",
    "          {miss_html}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        Marker(\n",
    "            location=[r[\"lat\"], r[\"lon\"]],\n",
    "            popup=Popup(popup_html, max_width=320),\n",
    "            icon=Icon(color=marker_color, icon=\"info-sign\")\n",
    "        ).add_to(feature_group)\n",
    "\n",
    "# ‚â§200 m lager\n",
    "toilets_pic_fg_0_200 = FeatureGroup(name=\"Toaletter ‚â§200 m (bild, Commons, Mapillary, saknade taggar)\")\n",
    "add_markers(basic_df_0_200, toilets_pic_fg_0_200, marker_color=\"darkblue\")\n",
    "toilets_pic_fg_0_200.add_to(m)\n",
    "\n",
    "# 200‚Äì400 m lager\n",
    "toilets_pic_fg_200_400 = FeatureGroup(name=\"Toaletter 200‚Äì400 m (bild, Commons, Mapillary, saknade taggar)\")\n",
    "add_markers(basic_df_200_400, toilets_pic_fg_200_400, marker_color=\"orange\")\n",
    "toilets_pic_fg_200_400.add_to(m)\n",
    "\n",
    "LayerControl(collapsed=True).add_to(m)  \n",
    "# =========================\n",
    "# 11) Spara karta + output\n",
    "# =========================\n",
    "map_html = f\"../kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_{timestamp}.html\"\n",
    "m.save(map_html)\n",
    "\n",
    "print(\"‚úÖ Klart!\")\n",
    "print(f\"‚Ä¢ Summary CSV (‚â§200 m): {summary_csv}\")\n",
    "print(f\"‚Ä¢ Toilets GeoJSON ‚â§200 m: {toilets_geojson_0_200}\")\n",
    "print(f\"‚Ä¢ Toilets CSV ‚â§200 m: {toilets_csv_0_200}\")\n",
    "print(f\"‚Ä¢ Toilets GeoJSON 200‚Äì400 m: {toilets_geojson_200_400}\")\n",
    "print(f\"‚Ä¢ Toilets CSV 200‚Äì400 m: {toilets_csv_200_400}\")\n",
    "print(f\"‚Ä¢ Basic CSV ‚â§200 m: {basic_csv_0_200}\")\n",
    "print(f\"‚Ä¢ Basic CSV 200‚Äì400 m: {basic_csv_200_400}\")\n",
    "print(f\"‚Ä¢ Karta: {map_html}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4961e3a-53b4-4169-a567-e38be677b433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2025-08-27 16:54:00\n",
      "Total time elapsed: 12.28 seconds\n"
     ]
    }
   ],
   "source": [
    " # End timer and calculate duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print current date and total time\n",
    "print(\"Date:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e43025-5c59-4cb8-be5b-6cbe7910eb62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
