{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79dc2ee3-82e9-44fe-9c85-e0f49b8a35e8",
   "metadata": {},
   "source": [
    "# Issue 132 Notebook Toaletter n√§ra SAT\n",
    "* denna [Notebook](https://github.com/salgo60/Stockholm_Archipelago_Trail/blob/main/notebook/Issue_132_Notebook_Toaletter_n%C3%A4ra_SAT.ipynb)\n",
    "* [Issue 132](https://github.com/salgo60/Stockholm_Archipelago_Trail/issues/132)\n",
    "\n",
    "Se liknande l√∂sning f√∂r Roslagsleden\n",
    "* nu har vi SAT = wikidata [Q131318799](https://www.wikidata.org/wiki/Q131318799)\n",
    "* \"leden\" sitter inte ihop utan varje √∂ har sitt segment\n",
    "\n",
    "Jmf dricksvatten [Issue 139](https://github.com/salgo60/Stockholm_Archipelago_Trail/issues/139) \n",
    "\n",
    "\n",
    "Output \n",
    "* [kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_07_28_17_12.html](https://raw.githack.com/salgo60/Stockholm_Archipelago_Trail/main/kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_07_28_17_12.html)\n",
    "* [kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_07_30_04_22.html](https://raw.githack.com/salgo60/Stockholm_Archipelago_Trail/main/kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_07_30_04_22.html)\n",
    "* [kartor/Issue_132_2_toaletter_nara_stockholm_archipelago_trail_2025_07_30_07_16.html](https://raw.githack.com/salgo60/Stockholm_Archipelago_Trail/main/kartor/Issue_132_2_toaletter_nara_stockholm_archipelago_trail_2025_07_30_07_16.html)\n",
    "* [kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_17_19_29.html](https://raw.githack.com/salgo60/Stockholm_Archipelago_Trail/main/kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_17_19_29.html)\n",
    "* [kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_19_16_01.html](https://raw.githack.com/salgo60/Stockholm_Archipelago_Trail/main/kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_19_16_01.html)\n",
    "* [kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_19_23_14.html](https://raw.githack.com/salgo60/Stockholm_Archipelago_Trail/main/kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_19_23_14.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd494e0-4277-42cd-8f35-d97c83229c07",
   "metadata": {},
   "source": [
    "version 0.1\n",
    "* unary_union --> union_all\n",
    "* start updating OSM with pictures from https://commons.wikimedia.org/wiki/Category:SAT_Todo\n",
    "* also count number of seats ie. toilets:number\n",
    "* Fetch SAT etapper via Wikidata\n",
    "* Fetch geometries per relation via Overpass (no zip-mismatch)\n",
    "* Buffer 200 m (Shapely ‚â•2 with union_all())\n",
    "* Fetch toilets as nodes/ways/relations via nwr[...] out center;\n",
    "* Count toilets via toilets:number (fallback male/female/unisex ‚Üí else 1)\n",
    "* Join to nearest etapp, build summary (CSV)\n",
    "* Build a Folium map with etapper, 200 m buffer, and toilet markers\n",
    "* Export toilets within 200 m as GeoJSON + CSV and save the map HTML\n",
    "\n",
    "version 0.2\n",
    "* added mapillary\n",
    "* added toilets:paper_supplied\n",
    "* getting error fetching OSM with overpass moved it to one call instead one per segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98b8e5d-53c1-4e11-b150-d75fdeabad97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2025-08-20 14:41:58\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "timestamp = now.timestamp()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Start:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d9fdf5-e47f-46be-afbb-2b0735aed757",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f7482-ba87-4792-90a7-9ec49bc664f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c3da576-6769-4e71-8cc2-12e6b2c9a7a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48bba8a7-e5cc-43ba-8b1a-eaaf61b1e780",
   "metadata": {},
   "source": [
    "New code 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc7bd02-1fc5-444d-b0fa-fcea69856888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç H√§mtar SAT-etapper fr√•n Wikidata...\n",
      "‚úÖ Hittade 20 etapper med OSM-relationer\n",
      "üì° H√§mtar geometrier fr√•n Overpass (robust, inkl. n√§stlade relationer) ...\n",
      "‚ùå Misslyckades att h√§mta batch: [19012436, 19020231, 19013576, 19079703, 19013473, 19014571, 19015969, 19012684, 19012654, 19023687, 19016280, 19014515]\n",
      "‚ùå Misslyckades att h√§mta batch: [19141225, 19013472, 19080874, 19018272, 19020310, 19023630, 19016187, 19081125]\n",
      "‚úÖ Klart. Relationer: 20 | Linjesegment totalt: 0\n",
      "‚ö†Ô∏è Overpass svarade utan way-geometrier. Prova att s√§nka CHUNK_SIZE till 6‚Äì8, eller k√∂r igen senare (serverlast).\n",
      "üß© Etapper med geometri: 0 / 20 (saknade/ogiltiga: 20)\n",
      "üßÆ Skapar 200 m-buffert och 200‚Äì400 m-ring...\n",
      "‚ö†Ô∏è Ingen trail-geometri ‚Äî hoppar √∂ver toalettfr√•gan.\n",
      "‚úÖ 0 toalett-objekt inom/vid 200 m\n",
      "‚úÖ 0 toalett-objekt inom 200‚Äì400 m\n",
      "üìä Summary (‚â§200 m):\n",
      "Empty DataFrame\n",
      "Columns: [label, island, sites, toilets_total, avg_distance_m, avg_toilets_per_site]\n",
      "Index: []\n",
      "üß≠ Map center: [59.33, 18.06]\n",
      "üßæ Basic CSV (‚â§200 m): ../kartor/sat_toaletter_basic_tags_0_200_2025_08_20_14_42.csv\n",
      "üßæ Basic CSV (200‚Äì400 m): ../kartor/sat_toaletter_basic_tags_200_400_2025_08_20_14_42.csv\n",
      "‚úÖ Klart!\n",
      "‚Ä¢ Summary CSV (‚â§200 m): ../kartor/sat_toaletter_summary_2025_08_20_14_42.csv\n",
      "‚Ä¢ Toilets GeoJSON ‚â§200 m: ../kartor/sat_toaletter_inrange_0_200_2025_08_20_14_42.geojson\n",
      "‚Ä¢ Toilets CSV ‚â§200 m: ../kartor/sat_toaletter_inrange_0_200_2025_08_20_14_42.csv\n",
      "‚Ä¢ Toilets GeoJSON 200‚Äì400 m: ../kartor/sat_toaletter_inrange_200_400_2025_08_20_14_42.geojson\n",
      "‚Ä¢ Toilets CSV 200‚Äì400 m: ../kartor/sat_toaletter_inrange_200_400_2025_08_20_14_42.csv\n",
      "‚Ä¢ Basic CSV ‚â§200 m: ../kartor/sat_toaletter_basic_tags_0_200_2025_08_20_14_42.csv\n",
      "‚Ä¢ Basic CSV 200‚Äì400 m: ../kartor/sat_toaletter_basic_tags_200_400_2025_08_20_14_42.csv\n",
      "‚Ä¢ Karta: ../kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_2025_08_20_14_42.html\n"
     ]
    }
   ],
   "source": [
    "# !pip install geopandas shapely folium requests SPARQLWrapper --quiet\n",
    "\n",
    "import os, re, requests, html\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import LineString, MultiLineString, Point, mapping\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import folium\n",
    "from folium import Marker, Icon, FeatureGroup, LayerControl, Popup\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================\n",
    "# 1) H√§mta SAT-etapper via Wikidata\n",
    "# =========================\n",
    "print(\"üîç H√§mtar SAT-etapper fr√•n Wikidata...\")\n",
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "sparql.setQuery(\"\"\"\n",
    "SELECT ?item ?itemLabel ?islandLabel ?osmid WHERE {\n",
    "  ?item wdt:P361 wd:Q131318799;\n",
    "        wdt:P31 wd:Q2143825;\n",
    "        wdt:P402 ?osmid.\n",
    "  OPTIONAL { ?item wdt:P706 ?island. }\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"sv,en\". }\n",
    "}\n",
    "\"\"\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "etapper = []\n",
    "for r in results[\"results\"][\"bindings\"]:\n",
    "    etapper.append({\n",
    "        \"id\": r[\"osmid\"][\"value\"],\n",
    "        \"label\": r.get(\"itemLabel\", {}).get(\"value\", \"\"),\n",
    "        \"island\": r.get(\"islandLabel\", {}).get(\"value\", \"\")\n",
    "    })\n",
    "osm_ids = [e['id'] for e in etapper]\n",
    "print(f\"‚úÖ Hittade {len(osm_ids)} etapper med OSM-relationer\")\n",
    "\n",
    "# =========================\n",
    "# 2) H√§mta geometrier per relation via Overpass (batched) + tags\n",
    "# =========================\n",
    "# =========================\n",
    "# 2) H√§mta geometrier per relation via Overpass (robust, nested rels, Mapillary)\n",
    "# =========================\n",
    "print(\"üì° H√§mtar geometrier fr√•n Overpass (robust, inkl. n√§stlade relationer) ...\")\n",
    "\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from collections import defaultdict, deque\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "# Try a few endpoints; we‚Äôll stick to the first that works\n",
    "OVERPASS_ENDPOINTS = [\n",
    "    \"https://overpass.kumi.systems/api/interpreter\",\n",
    "    \"https://overpass-api.de/api/interpreter\",\n",
    "]\n",
    "overpass_url = None  # will be set to a working endpoint\n",
    "\n",
    "# Results\n",
    "geom_per_rel = {}   # {rel_id: [LineString, ...]}\n",
    "geom_per_rel_meta = defaultdict(lambda: {\"mapillary_urls\": set()})\n",
    "all_lines = []\n",
    "\n",
    "def _sleep_backoff(i):\n",
    "    wait = (2 ** i) + random.random()\n",
    "    time.sleep(wait)\n",
    "\n",
    "def _overpass_post(url, query, retries=3):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            r = requests.post(url, data={\"data\": query}, timeout=120)\n",
    "        except Exception:\n",
    "            r = None\n",
    "        if r is not None and r.status_code == 200:\n",
    "            ct = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "            if \"json\" in ct:\n",
    "                try:\n",
    "                    return r.json()\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if i < retries - 1:\n",
    "            _sleep_backoff(i)\n",
    "    return None\n",
    "\n",
    "def _pick_working_endpoint(test_query):\n",
    "    for ep in OVERPASS_ENDPOINTS:\n",
    "        data = _overpass_post(ep, test_query, retries=2)\n",
    "        if isinstance(data, dict):\n",
    "            return ep\n",
    "    return OVERPASS_ENDPOINTS[0]  # fallback anyway\n",
    "\n",
    "# Minimal test to pick an endpoint\n",
    "_test_q = \"\"\"\n",
    "[out:json][timeout:25];\n",
    "node(0,0,0,0);\n",
    "out;\n",
    "\"\"\"\n",
    "overpass_url = _pick_working_endpoint(_test_q)\n",
    "\n",
    "def _chunks(iterable, n):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        batch = []\n",
    "        try:\n",
    "            for _ in range(n):\n",
    "                batch.append(next(it))\n",
    "        except StopIteration:\n",
    "            pass\n",
    "        if not batch:\n",
    "            break\n",
    "        yield batch\n",
    "\n",
    "CHUNK_SIZE = 12  # slightly smaller to avoid timeouts\n",
    "\n",
    "# Pre-init relations\n",
    "for rid in osm_ids:\n",
    "    geom_per_rel.setdefault(int(rid), [])\n",
    "\n",
    "# Helper to collect Mapillary pKeys ‚Üí URLs\n",
    "def _mapillary_urls_from_tags(tags):\n",
    "    raw = (tags or {}).get(\"mapillary\", \"\")\n",
    "    if not raw or not str(raw).strip():\n",
    "        return []\n",
    "    keys = [k for k in re.split(r\"[;,\\s]+\", str(raw).strip()) if k]\n",
    "    return [f\"https://www.mapillary.com/app/?pKey={k}\" for k in keys]\n",
    "\n",
    "for batch_ids in _chunks([int(x) for x in osm_ids], CHUNK_SIZE):\n",
    "    ids_str = \",\".join(map(str, batch_ids))\n",
    "\n",
    "    # Key change:\n",
    "    #  - Get relations WITH members (out body) so we can map rel‚Üísubrels/ways\n",
    "    #  - Recurse to include nested relations\n",
    "    #  - Output ALL ways (out geom) so we actually get geometries\n",
    "    q = f\"\"\"\n",
    "    [out:json][timeout:180];\n",
    "    rel(id:{ids_str})->.R;\n",
    "    .R out body;\n",
    "    (.R; >>;)->.ALL;         // recurse down (members of rels, including nested)\n",
    "    way.ALL out geom tags;   // output ways with geometry + tags\n",
    "    \"\"\"\n",
    "\n",
    "    data = _overpass_post(overpass_url, q, retries=3)\n",
    "    if not isinstance(data, dict):\n",
    "        # Try the other endpoint once for this batch\n",
    "        for ep in OVERPASS_ENDPOINTS:\n",
    "            if ep == overpass_url:\n",
    "                continue\n",
    "            data = _overpass_post(ep, q, retries=2)\n",
    "            if isinstance(data, dict):\n",
    "                overpass_url = ep  # switch for subsequent calls\n",
    "                break\n",
    "        if not isinstance(data, dict):\n",
    "            print(f\"‚ùå Misslyckades att h√§mta batch: {batch_ids}\")\n",
    "            continue\n",
    "\n",
    "    elements = data.get(\"elements\", []) or []\n",
    "\n",
    "    # Separate elements by type for clarity\n",
    "    rel_elems = [e for e in elements if e.get(\"type\") == \"relation\"]\n",
    "    way_elems = [e for e in elements if e.get(\"type\") == \"way\"]\n",
    "\n",
    "    # Map: relation -> {child_rel_ids}, relation -> {member_way_ids}\n",
    "    rel_children = defaultdict(set)\n",
    "    rel_direct_ways = defaultdict(set)\n",
    "\n",
    "    for rel in rel_elems:\n",
    "        rid = rel.get(\"id\")\n",
    "        for m in rel.get(\"members\", []) or []:\n",
    "            if m.get(\"type\") == \"relation\":\n",
    "                rel_children[rid].add(m.get(\"ref\"))\n",
    "            elif m.get(\"type\") == \"way\":\n",
    "                rel_direct_ways[rid].add(m.get(\"ref\"))\n",
    "\n",
    "    # Map: way_id -> LineString (+ Mapillary links)\n",
    "    way_geom = {}\n",
    "    way_mapillary = defaultdict(list)\n",
    "    for w in way_elems:\n",
    "        if \"geometry\" not in w:\n",
    "            continue\n",
    "        coords = [(pt[\"lon\"], pt[\"lat\"]) for pt in w[\"geometry\"] if \"lon\" in pt and \"lat\" in pt]\n",
    "        if len(coords) < 2:\n",
    "            continue\n",
    "        try:\n",
    "            line = LineString(coords)\n",
    "        except Exception:\n",
    "            continue\n",
    "        way_geom[w[\"id\"]] = line\n",
    "        for url in _mapillary_urls_from_tags(w.get(\"tags\", {})):\n",
    "            way_mapillary[w[\"id\"]].append(url)\n",
    "\n",
    "    # For each top-level relation in this batch, collect ALL descendant ways via BFS\n",
    "    for top_rel in batch_ids:\n",
    "        top_rel = int(top_rel)\n",
    "\n",
    "        # BFS through nested relations\n",
    "        visited = set()\n",
    "        stack = deque([top_rel])\n",
    "        all_way_ids = set()\n",
    "\n",
    "        while stack:\n",
    "            current = stack.popleft()\n",
    "            if current in visited:\n",
    "                continue\n",
    "            visited.add(current)\n",
    "            # direct ways\n",
    "            all_way_ids |= rel_direct_ways.get(current, set())\n",
    "            # traverse children relations\n",
    "            for child in rel_children.get(current, set()):\n",
    "                if child not in visited:\n",
    "                    stack.append(child)\n",
    "\n",
    "        # Build LineStrings + Mapillary URLs\n",
    "        rel_lines = []\n",
    "        for wid in all_way_ids:\n",
    "            geom = way_geom.get(wid)\n",
    "            if geom is not None:\n",
    "                rel_lines.append(geom)\n",
    "                all_lines.append(geom)\n",
    "                for u in way_mapillary.get(wid, []):\n",
    "                    geom_per_rel_meta[top_rel][\"mapillary_urls\"].add(u)\n",
    "\n",
    "        if rel_lines:\n",
    "            geom_per_rel[top_rel].extend(rel_lines)\n",
    "\n",
    "    # be gentle between batches\n",
    "    time.sleep(1.0)\n",
    "\n",
    "# --- Summering ---\n",
    "antal_rel = len(geom_per_rel)\n",
    "antal_linjer = len(all_lines)\n",
    "print(f\"‚úÖ Klart. Relationer: {antal_rel} | Linjesegment totalt: {antal_linjer}\")\n",
    "if antal_linjer == 0:\n",
    "    print(\"‚ö†Ô∏è Overpass svarade utan way-geometrier. Prova att s√§nka CHUNK_SIZE till 6‚Äì8, eller k√∂r igen senare (serverlast).\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2b) Bygg etapp-GeoDataFrame + samlad trail (robust)\n",
    "# =========================\n",
    "from shapely.ops import linemerge, unary_union\n",
    "from shapely.geometry import MultiLineString\n",
    "\n",
    "def _normalize_lines(lines):\n",
    "    \"\"\"\n",
    "    Tar en lista av LineStrings och returnerar LineString eller MultiLineString.\n",
    "    Rensar bort icke-line√§ra geometrier om de dyker upp.\n",
    "    \"\"\"\n",
    "    if not lines:\n",
    "        return None\n",
    "    try:\n",
    "        merged = linemerge(unary_union(lines))\n",
    "    except Exception:\n",
    "        try:\n",
    "            merged = linemerge(MultiLineString(lines))\n",
    "        except Exception:\n",
    "            return None\n",
    "    gtype = getattr(merged, \"geom_type\", \"\")\n",
    "    if gtype in (\"LineString\", \"MultiLineString\"):\n",
    "        return merged\n",
    "    if gtype == \"GeometryCollection\":\n",
    "        only_lines = [g for g in merged.geoms if g.geom_type == \"LineString\"]\n",
    "        if not only_lines:\n",
    "            return None\n",
    "        return only_lines[0] if len(only_lines) == 1 else MultiLineString(only_lines)\n",
    "    return None\n",
    "\n",
    "meta_rows = []\n",
    "count_no_lines = 0\n",
    "\n",
    "for et in etapper:\n",
    "    rel_id = int(et[\"id\"])\n",
    "    lines = geom_per_rel.get(rel_id, []) or []\n",
    "    if not lines:\n",
    "        count_no_lines += 1\n",
    "        continue\n",
    "    norm = _normalize_lines(lines)\n",
    "    if norm is None:\n",
    "        count_no_lines += 1\n",
    "        continue\n",
    "    meta_rows.append({\n",
    "        \"rel_id\": rel_id,\n",
    "        \"label\": et.get(\"label\", \"\"),\n",
    "        \"island\": et.get(\"island\", \"\"),\n",
    "        \"geometry\": norm\n",
    "    })\n",
    "\n",
    "if meta_rows:\n",
    "    meta_gdf = gpd.GeoDataFrame(meta_rows, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "else:\n",
    "    meta_gdf = gpd.GeoDataFrame(\n",
    "        {\n",
    "            \"rel_id\": pd.Series(dtype=\"int64\"),\n",
    "            \"label\": pd.Series(dtype=\"object\"),\n",
    "            \"island\": pd.Series(dtype=\"object\"),\n",
    "        },\n",
    "        geometry=gpd.GeoSeries([], dtype=\"geometry\", crs=\"EPSG:4326\"),\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "\n",
    "gdf_trail = meta_gdf[[\"geometry\"]].copy()\n",
    "\n",
    "print(f\"üß© Etapper med geometri: {len(meta_gdf)} / {len(etapper)} (saknade/ogiltiga: {count_no_lines})\")\n",
    "# =========================\n",
    "# 3) Buffert 200 m + ring 200‚Äì400 m  (PATCHED)\n",
    "# =========================\n",
    "print(\"üßÆ Skapar 200 m-buffert och 200‚Äì400 m-ring...\")\n",
    "trail_utm = gdf_trail.to_crs(3006)\n",
    "buffer_utm_200 = trail_utm.buffer(200)\n",
    "buffer_utm_400 = trail_utm.buffer(400)\n",
    "\n",
    "# Use union_all() when available (Shapely 2), otherwise unary_union (older)\n",
    "buffer_utm_200_u = buffer_utm_200.union_all() if hasattr(buffer_utm_200, \"union_all\") else buffer_utm_200.unary_union\n",
    "buffer_utm_400_u = buffer_utm_400.union_all() if hasattr(buffer_utm_400, \"union_all\") else buffer_utm_400.unary_union\n",
    "\n",
    "# Ring = 400m minus 200m\n",
    "ring_utm = buffer_utm_400_u.difference(buffer_utm_200_u)\n",
    "\n",
    "# Tillbaka till WGS84\n",
    "buffer_union = gpd.GeoSeries([buffer_utm_200_u], crs=3006).to_crs(4326).iloc[0]\n",
    "ring_union   = gpd.GeoSeries([ring_utm],       crs=3006).to_crs(4326).iloc[0]\n",
    "\n",
    "# Tillbaka till WGS84\n",
    "buffer_union = gpd.GeoSeries([buffer_utm_200_u], crs=3006).to_crs(4326).iloc[0]\n",
    "ring_union = gpd.GeoSeries([ring_utm], crs=3006).to_crs(4326).iloc[0]\n",
    "\n",
    "# =========================\n",
    "# 4) H√§mta toaletter (robust Overpass)  (PATCHED)\n",
    "# =========================\n",
    "def _parse_int(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    m = re.search(r\"\\d+\", str(v))\n",
    "    return int(m.group()) if m else None\n",
    "\n",
    "def toilets_count_from_tags(tags: dict) -> int:\n",
    "    \"\"\"\n",
    "    Prioritet f√∂r antal: toilets:num_chambers ‚Üí toilets:number ‚Üí sum av male/female/unisex.\n",
    "    \"\"\"\n",
    "    n = _parse_int((tags or {}).get(\"toilets:num_chambers\"))\n",
    "    if n is not None:\n",
    "        return n\n",
    "    n2 = _parse_int((tags or {}).get(\"toilets:number\"))\n",
    "    if n2 is not None:\n",
    "        return n2\n",
    "    parts = [\n",
    "        _parse_int((tags or {}).get(\"male:toilets\")),\n",
    "        _parse_int((tags or {}).get(\"female:toilets\")),\n",
    "        _parse_int((tags or {}).get(\"unisex:toilets\")),\n",
    "    ]\n",
    "    parts = [p for p in parts if p is not None]\n",
    "    return sum(parts) if parts else 1\n",
    "\n",
    "# Skip if no trail geometry\n",
    "if gdf_trail.empty:\n",
    "    print(\"‚ö†Ô∏è Ingen trail-geometri ‚Äî hoppar √∂ver toalettfr√•gan.\")\n",
    "    gdf_toilets = gpd.GeoDataFrame(columns=[\"geometry\",\"tags\",\"id\",\"osm_type\",\"toilets_num\"], crs=\"EPSG:4326\")\n",
    "else:\n",
    "    bbox = gdf_trail.total_bounds  # [minx, miny, maxx, maxy]\n",
    "    q_toilets = f\"\"\"\n",
    "    [out:json][timeout:60];\n",
    "    nwr[\"amenity\"=\"toilets\"]({bbox[1]},{bbox[0]},{bbox[3]},{bbox[2]});\n",
    "    out center;\n",
    "    \"\"\"\n",
    "\n",
    "    def overpass_json(query, retries=4):\n",
    "        resp = overpass_request(query, retries=retries)  # uses your existing helper\n",
    "        if resp is None:\n",
    "            return None, \"No response from Overpass (after retries)\"\n",
    "        ct = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
    "        if \"json\" not in ct:\n",
    "            head = resp.text[:300].replace(\"\\n\", \" \")\n",
    "            return None, f\"Unexpected content-type: {ct}. Head: {head}\"\n",
    "        try:\n",
    "            data = resp.json()\n",
    "            return data, None\n",
    "        except Exception as e:\n",
    "            head = resp.text[:300].replace(\"\\n\", \" \")\n",
    "            return None, f\"JSON decode error: {e}. Head: {head}\"\n",
    "\n",
    "    print(\"üöΩ H√§mtar toaletter (nwr) fr√•n Overpass...\")\n",
    "    data, err = overpass_json(q_toilets, retries=4)\n",
    "    if err:\n",
    "        print(f\"‚ùå Overpass (toilets) failed: {err}\")\n",
    "        elements = []\n",
    "    else:\n",
    "        elements = data.get(\"elements\", []) if isinstance(data, dict) else []\n",
    "\n",
    "    toilets = []\n",
    "    for el in elements:\n",
    "        tags = el.get(\"tags\", {}) or {}\n",
    "        typ = el.get(\"type\")\n",
    "        if typ == \"node\":\n",
    "            lon, lat = el.get(\"lon\"), el.get(\"lat\")\n",
    "        else:\n",
    "            center = el.get(\"center\") or {}\n",
    "            lon, lat = center.get(\"lon\"), center.get(\"lat\")\n",
    "        if lon is None or lat is None:\n",
    "            continue\n",
    "        toilets.append({\n",
    "            \"geometry\": Point(lon, lat),\n",
    "            \"tags\": tags,\n",
    "            \"id\": el.get(\"id\"),\n",
    "            \"osm_type\": typ,\n",
    "            \"toilets_num\": toilets_count_from_tags(tags),\n",
    "        })\n",
    "\n",
    "    gdf_toilets = gpd.GeoDataFrame(toilets, crs=\"EPSG:4326\")\n",
    "    print(f\"‚úÖ Hittade {len(gdf_toilets)} toalett-objekt inom bbox\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Dela upp: inom ‚â§200 m och i ringen 200‚Äì400 m\n",
    "# =========================\n",
    "in_0_200 = gdf_toilets[gdf_toilets.geometry.covered_by(buffer_union)]\n",
    "in_200_400 = gdf_toilets[gdf_toilets.geometry.covered_by(ring_union)]\n",
    "print(f\"‚úÖ {len(in_0_200)} toalett-objekt inom/vid 200 m\")\n",
    "print(f\"‚úÖ {len(in_200_400)} toalett-objekt inom 200‚Äì400 m\")\n",
    "\n",
    "# =========================\n",
    "# 6) N√§rmaste etapp per kategori\n",
    "# =========================\n",
    "meta_utm = meta_gdf.to_crs(3006)\n",
    "toilets_utm_0_200 = in_0_200.to_crs(3006)\n",
    "toilets_utm_200_400 = in_200_400.to_crs(3006)\n",
    "\n",
    "joined_0_200 = gpd.sjoin_nearest(\n",
    "    toilets_utm_0_200,\n",
    "    meta_utm[[\"label\", \"island\", \"geometry\"]],\n",
    "    how=\"left\",\n",
    "    distance_col=\"distance_m\"\n",
    ").to_crs(4326)\n",
    "\n",
    "joined_200_400 = gpd.sjoin_nearest(\n",
    "    toilets_utm_200_400,\n",
    "    meta_utm[[\"label\", \"island\", \"geometry\"]],\n",
    "    how=\"left\",\n",
    "    distance_col=\"distance_m\"\n",
    ").to_crs(4326)\n",
    "\n",
    "# =========================\n",
    "# 7) Summary f√∂r ‚â§200 m\n",
    "# =========================\n",
    "summary = (\n",
    "    joined_0_200.assign(toilets_num=joined_0_200[\"toilets_num\"].fillna(1))\n",
    "    .groupby([\"label\", \"island\"], as_index=False)\n",
    "    .agg(\n",
    "        sites=(\"geometry\", \"count\"),\n",
    "        toilets_total=(\"toilets_num\", \"sum\"),\n",
    "        avg_distance_m=(\"distance_m\", \"mean\"),\n",
    "    )\n",
    "    .assign(avg_toilets_per_site=lambda df: df[\"toilets_total\"] / df[\"sites\"])\n",
    "    .sort_values([\"toilets_total\", \"sites\"], ascending=[False, False])\n",
    ")\n",
    "print(\"üìä Summary (‚â§200 m):\")\n",
    "print(summary.head(1000))\n",
    "\n",
    "# -------------------------\n",
    "# Helper: robust map center\n",
    "# -------------------------\n",
    "def compute_map_center(gdf_trail, gdf_toilets=None, default=(59.33, 18.06)):\n",
    "    \"\"\"\n",
    "    Returns [lat, lon] for map center.\n",
    "    Tries trail union centroid -> trail bbox -> toilets bbox -> default.\n",
    "    Safe for empty geometries.\n",
    "    \"\"\"\n",
    "    # 1) Trail union centroid\n",
    "    try:\n",
    "        if gdf_trail is not None and not gdf_trail.empty:\n",
    "            geom = gdf_trail.geometry\n",
    "            union_geom = geom.union_all() if hasattr(geom, \"union_all\") else geom.unary_union\n",
    "            if union_geom and not union_geom.is_empty:\n",
    "                c = union_geom.centroid\n",
    "                if c and not c.is_empty:\n",
    "                    return [float(c.y), float(c.x)]\n",
    "            # 2) Trail bbox center\n",
    "            minx, miny, maxx, maxy = gdf_trail.total_bounds\n",
    "            if (maxx > minx) and (maxy > miny):\n",
    "                return [float((miny + maxy) / 2.0), float((minx + maxx) / 2.0)]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) Toilets bbox center\n",
    "    try:\n",
    "        if gdf_toilets is not None and not gdf_toilets.empty:\n",
    "            minx, miny, maxx, maxy = gdf_toilets.total_bounds\n",
    "            if (maxx > minx) and (maxy > miny):\n",
    "                return [float((miny + maxy) / 2.0), float((minx + maxx) / 2.0)]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4) Fallback: Stockholm-ish\n",
    "    return [float(default[0]), float(default[1])]\n",
    "\n",
    "# =========================\n",
    "# 8) Spara filer + grundkarta\n",
    "# =========================\n",
    "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "os.makedirs(\"../kartor\", exist_ok=True)\n",
    "\n",
    "summary_csv = f\"../kartor/sat_toaletter_summary_{timestamp}.csv\"\n",
    "summary.to_csv(summary_csv, index=False)\n",
    "\n",
    "toilets_geojson_0_200 = f\"../kartor/sat_toaletter_inrange_0_200_{timestamp}.geojson\"\n",
    "toilets_csv_0_200 = f\"../kartor/sat_toaletter_inrange_0_200_{timestamp}.csv\"\n",
    "in_0_200[[\"id\", \"osm_type\", \"toilets_num\", \"tags\", \"geometry\"]].to_file(toilets_geojson_0_200, driver=\"GeoJSON\")\n",
    "in_0_200.drop(columns=\"geometry\").to_csv(toilets_csv_0_200, index=False)\n",
    "\n",
    "toilets_geojson_200_400 = f\"../kartor/sat_toaletter_inrange_200_400_{timestamp}.geojson\"\n",
    "toilets_csv_200_400 = f\"../kartor/sat_toaletter_inrange_200_400_{timestamp}.csv\"\n",
    "in_200_400[[\"id\", \"osm_type\", \"toilets_num\", \"tags\", \"geometry\"]].to_file(toilets_geojson_200_400, driver=\"GeoJSON\")\n",
    "in_200_400.drop(columns=\"geometry\").to_csv(toilets_csv_200_400, index=False)\n",
    "\n",
    "# Bygg karta\n",
    "center_latlon = compute_map_center(gdf_trail, gdf_toilets, default=(59.33, 18.06))\n",
    "m = folium.Map(location=center_latlon, zoom_start=9, control_scale=True)\n",
    "print(\"üß≠ Map center:\", center_latlon) \n",
    "\n",
    "# Etapper\n",
    "colors = [\n",
    "    \"blue\",\"green\",\"purple\",\"orange\",\"darkred\",\"cadetblue\",\"lightgray\",\"darkblue\",\n",
    "    \"darkgreen\",\"pink\",\"lightblue\",\"lightgreen\",\"gray\",\"black\",\"beige\",\"lightred\"\n",
    "]\n",
    "for i, row in meta_gdf.reset_index(drop=True).iterrows():\n",
    "    color = colors[i % len(colors)]\n",
    "    popup = f\"<b>{html.escape(row['label'])}</b><br>√ñ: {html.escape(row['island'])}\"\n",
    "    folium.GeoJson(\n",
    "        data=mapping(row.geometry),\n",
    "        name=row[\"label\"],\n",
    "        style_function=lambda x, c=color: {\"color\": c, \"weight\": 3}\n",
    "    ).add_child(folium.Popup(popup, max_width=350)).add_to(m)\n",
    "\n",
    "# Buffert-lager (200 m och 200‚Äì400 m)\n",
    "folium.GeoJson(\n",
    "    data=mapping(buffer_union),\n",
    "    name=\"200 m Buffert\",\n",
    "    style_function=lambda x: {'fillColor': '#0000ff', 'color': '#0000ff', 'weight': 1, 'fillOpacity': 0.1}\n",
    ").add_to(m)\n",
    "\n",
    "folium.GeoJson(\n",
    "    data=mapping(ring_union),\n",
    "    name=\"Ring 200‚Äì400 m\",\n",
    "    style_function=lambda x: {'fillColor': '#ffa500', 'color': '#ffa500', 'weight': 1, 'fillOpacity': 0.1}\n",
    ").add_to(m)\n",
    "\n",
    "# =========================\n",
    "# 9) Commons helpers (support Category + avoid double-encoding)\n",
    "# =========================\n",
    "def _quote_commons(title: str) -> str:\n",
    "    return requests.utils.quote(title, safe=':/%')\n",
    "\n",
    "def commons_title_to_page(title: str) -> str:\n",
    "    if not title:\n",
    "        return None\n",
    "    t = title.strip()\n",
    "    low = t.lower()\n",
    "    if low.startswith((\"category:\", \"file:\", \"image:\")):\n",
    "        return f\"https://commons.wikimedia.org/wiki/{_quote_commons(t)}\"\n",
    "    return f\"https://commons.wikimedia.org/wiki/{_quote_commons('File:' + t)}\"\n",
    "\n",
    "def commons_title_to_filepath(title: str, thumb_width=400) -> str | None:\n",
    "    if not title:\n",
    "        return None\n",
    "    t = title.strip()\n",
    "    low = t.lower()\n",
    "    if low.startswith(\"category:\"):\n",
    "        return None\n",
    "    if not low.startswith((\"file:\", \"image:\")):\n",
    "        t = \"File:\" + t\n",
    "    return f\"https://commons.wikimedia.org/wiki/Special:FilePath/{_quote_commons(t)}?width={thumb_width}\"\n",
    "\n",
    "_wikidata_image_cache = {}\n",
    "def wikidata_p18_thumb(qid: str, thumb_width=400) -> str | None:\n",
    "    if not qid:\n",
    "        return None\n",
    "    qid = qid.strip()\n",
    "    if qid in _wikidata_image_cache:\n",
    "        return _wikidata_image_cache[qid]\n",
    "    try:\n",
    "        url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
    "        data = requests.get(url, timeout=15).json()\n",
    "        ent = data.get(\"entities\", {}).get(qid, {})\n",
    "        p18 = ent.get(\"claims\", {}).get(\"P18\", [])\n",
    "        if not p18:\n",
    "            _wikidata_image_cache[qid] = None\n",
    "            return None\n",
    "        filename = p18[0][\"mainsnak\"][\"datavalue\"][\"value\"]\n",
    "        img_url = commons_title_to_filepath(filename, thumb_width=thumb_width)\n",
    "        _wikidata_image_cache[qid] = img_url\n",
    "        return img_url\n",
    "    except Exception:\n",
    "        _wikidata_image_cache[qid] = None\n",
    "        return None\n",
    "\n",
    "def best_image_url_from_tags(tags: dict) -> str | None:\n",
    "    if not tags:\n",
    "        return None\n",
    "    if \"image\" in tags and str(tags[\"image\"]).strip():\n",
    "        first = str(tags[\"image\"]).split(\";\")[0].strip()\n",
    "        if first.lower().startswith((\"http://\", \"https://\")):\n",
    "            return first\n",
    "        return commons_title_to_filepath(first)\n",
    "    if \"wikimedia_commons\" in tags and str(tags[\"wikimedia_commons\"]).strip():\n",
    "        title = str(tags[\"wikimedia_commons\"]).strip()\n",
    "        thumb = commons_title_to_filepath(title)\n",
    "        if thumb:\n",
    "            return thumb\n",
    "    if \"wikidata\" in tags and str(tags[\"wikidata\"]).strip():\n",
    "        return wikidata_p18_thumb(tags[\"wikidata\"])\n",
    "    return None \n",
    "def compute_map_center(gdf_trail, gdf_toilets=None, default=(59.33, 18.06)):\n",
    "    \"\"\"\n",
    "    Returns [lat, lon] for map center.\n",
    "    Tries trail union centroid, then trail bbox, then toilets bbox, else default.\n",
    "    \"\"\"\n",
    "    # 1) Trail union centroid\n",
    "    if gdf_trail is not None and not gdf_trail.empty:\n",
    "        geom = gdf_trail.geometry\n",
    "        try:\n",
    "            union_geom = geom.union_all() if hasattr(geom, \"union_all\") else geom.unary_union\n",
    "        except Exception:\n",
    "            union_geom = None\n",
    "        if union_geom and not union_geom.is_empty:\n",
    "            c = union_geom.centroid\n",
    "            if c and not c.is_empty:\n",
    "                return [float(c.y), float(c.x)]\n",
    "        # 2) Trail bbox center\n",
    "        try:\n",
    "            minx, miny, maxx, maxy = gdf_trail.total_bounds\n",
    "            if (maxx > minx) and (maxy > miny):\n",
    "                return [float((miny + maxy) / 2.0), float((minx + maxx) / 2.0)]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3) Toilets bbox center\n",
    "    if gdf_toilets is not None and not gdf_toilets.empty:\n",
    "        try:\n",
    "            minx, miny, maxx, maxy = gdf_toilets.total_bounds\n",
    "            if (maxx > minx) and (maxy > miny):\n",
    "                return [float((miny + maxy) / 2.0), float((minx + maxx) / 2.0)]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 4) Fallback\n",
    "    return [float(default[0]), float(default[1])]\n",
    "    \n",
    "\n",
    "# === Mapillary helper (extract pKey links from tags) ===\n",
    "def mapillary_links_from_tags(tags: dict) -> list[str]:\n",
    "    if not tags or \"mapillary\" not in tags or not str(tags[\"mapillary\"]).strip():\n",
    "        return []\n",
    "    raw = str(tags[\"mapillary\"]).strip()\n",
    "    keys = [k for k in re.split(r\"[;,\\s]+\", raw) if k]\n",
    "    return [f\"https://www.mapillary.com/app/?pKey={k}\" for k in keys]\n",
    "\n",
    "# =========================\n",
    "# 10) Taggar & QA\n",
    "# ========================= \n",
    "BASIC_TAGS = [\n",
    "    \"amenity\", \"access\", \"opening_hours\", \"fee\", \"wheelchair\",\n",
    "    \"toilets:num_chambers\", \"unisex\", \"drinking_water\", \"changing_table\",\n",
    "    \"toilets:paper_supplied\", \"indoor\", \"operator\", \"website\", \"source\"\n",
    "]\n",
    "\n",
    "def missing_tags(tags: dict) -> list:\n",
    "    missing = []\n",
    "    tags = tags or {}\n",
    "    for k in BASIC_TAGS:\n",
    "        if k == \"amenity\":\n",
    "            if tags.get(\"amenity\") != \"toilets\":\n",
    "                missing.append(\"amenity=toilets\")\n",
    "            continue\n",
    "        v = tags.get(k)\n",
    "        if v is None or str(v).strip() == \"\":\n",
    "            missing.append(k)\n",
    "    if \"unisex\" in missing and ((\"male\" in tags or \"male:toilets\" in tags) and (\"female\" in tags or \"female:toilets\" in tags)):\n",
    "        try:\n",
    "            missing.remove(\"unisex\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return missing\n",
    "\n",
    "def build_basic_table(joined_df):\n",
    "    tag_cols = [\n",
    "    \"access\",\"opening_hours\",\"fee\",\"wheelchair\",\"toilets:num_chambers\",\"unisex\",\n",
    "    \"drinking_water\",\"changing_table\",\"toilets:paper_supplied\",\"indoor\",\"operator\",\"website\",\"source\",\n",
    "    \"wikidata\",\"wikimedia_commons\",\"image\",\"mapillary\"\n",
    "    ]\n",
    "    rows = []\n",
    "    for _, r in joined_df.to_crs(4326).iterrows():\n",
    "        tags = r.get(\"tags\", {}) or {}\n",
    "        commons_title = str(tags.get(\"wikimedia_commons\") or \"\").strip() or None\n",
    "        commons_page_url = commons_title_to_page(commons_title) if commons_title else None\n",
    "\n",
    "        img_url = best_image_url_from_tags(tags)\n",
    "        miss = missing_tags(tags)\n",
    "        mp_links = mapillary_links_from_tags(tags)\n",
    "\n",
    "        row = {\n",
    "            \"id\": r[\"id\"],\n",
    "            \"osm_type\": r[\"osm_type\"],\n",
    "            \"lat\": r.geometry.y,\n",
    "            \"lon\": r.geometry.x,\n",
    "            \"label\": r.get(\"label\", \"\"),\n",
    "            \"island\": r.get(\"island\", \"\"),\n",
    "            \"distance_m\": round(float(r.get(\"distance_m\", 0)), 1),\n",
    "            \"toilets_num\": int(r.get(\"toilets_num\", 1)),\n",
    "            \"image_url\": img_url,\n",
    "            \"missing_tags\": \", \".join(miss),\n",
    "            \"wikimedia_commons_title\": commons_title,\n",
    "            \"wikimedia_commons_url\": commons_page_url,\n",
    "            \"mapillary_links\": mp_links,\n",
    "        }\n",
    "        for k in tag_cols:\n",
    "            row[k] = (tags.get(k) if isinstance(tags, dict) else None)\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# =========================\n",
    "# 11) Bygg tabeller och lager f√∂r ‚â§200 m och 200‚Äì400 m\n",
    "# =========================\n",
    "basic_df_0_200 = build_basic_table(joined_0_200)\n",
    "basic_df_200_400 = build_basic_table(joined_200_400)\n",
    "\n",
    "basic_csv_0_200 = f\"../kartor/sat_toaletter_basic_tags_0_200_{timestamp}.csv\"\n",
    "basic_df_0_200.to_csv(basic_csv_0_200, index=False)\n",
    "basic_csv_200_400 = f\"../kartor/sat_toaletter_basic_tags_200_400_{timestamp}.csv\"\n",
    "basic_df_200_400.to_csv(basic_csv_200_400, index=False)\n",
    "\n",
    "print(\"üßæ Basic CSV (‚â§200 m):\", basic_csv_0_200)\n",
    "print(\"üßæ Basic CSV (200‚Äì400 m):\", basic_csv_200_400)\n",
    "\n",
    "def add_markers(df, feature_group, marker_color):\n",
    "    for _, r in df.iterrows():\n",
    "        osm_url = f\"https://www.openstreetmap.org/{'node' if r['osm_type']=='node' else 'way' if r['osm_type']=='way' else 'relation'}/{r['id']}\"\n",
    "        img_html = f'<div style=\"margin:6px 0\"><img src=\"{html.escape(str(r[\"image_url\"]))}\" referrerpolicy=\"no-referrer\" style=\"max-width:280px; height:auto; display:block;\"/></div>' if r.get(\"image_url\") else \"\"\n",
    "        miss_html = f\"<div style='color:#b00; margin-top:4px'><b>Saknade taggar:</b> {html.escape(str(r['missing_tags']))}</div>\" if r.get(\"missing_tags\") else \"\"\n",
    "        commons_html = \"\"\n",
    "        if r.get(\"wikimedia_commons_url\"):\n",
    "            title = r.get(\"wikimedia_commons_title\") or \"Wikimedia Commons\"\n",
    "            commons_html = f'<div style=\"margin-top:4px\">üì∑ <a href=\"{html.escape(str(r[\"wikimedia_commons_url\"]))}\" target=\"_blank\">{html.escape(str(title))}</a></div>'\n",
    "\n",
    "        # Mapillary l√§nkar\n",
    "        mp_links = r.get(\"mapillary_links\") or []\n",
    "        if isinstance(mp_links, str):\n",
    "            mp_links = [u for u in re.split(r\"[;\\s,]+\", mp_links) if u]\n",
    "        mapillary_html = \"\"\n",
    "        if mp_links:\n",
    "            items = \"\".join(f'<li><a href=\"{html.escape(u)}\" target=\"_blank\" rel=\"noopener\">√ñppna i Mapillary</a></li>' for u in mp_links[:5])\n",
    "            mapillary_html = f'<div style=\"margin-top:4px\">üó∫Ô∏è Mapillary:<ul style=\"margin:4px 0 0 18px\">{items}</ul></div>'\n",
    "        kv = []\n",
    "        for k in [\"opening_hours\",\"fee\",\"wheelchair\",\"access\",\"toilets:num_chambers\",\"unisex\",\"drinking_water\",\"changing_table\",\"toilets:paper_supplied\"]:\n",
    "            v = r.get(k)\n",
    "            if pd.notna(v) and str(v).strip() != \"\":\n",
    "                kv.append(f\"{k}={html.escape(str(v))}\")\n",
    "\n",
    "        kv_html = (\"<div>\" + \"<br>\".join(kv) + \"</div>\") if kv else \"\"\n",
    "\n",
    "        popup_html = f\"\"\"\n",
    "        <div style=\"font-size:13px; line-height:1.4\">\n",
    "          <b><a href=\"{osm_url}\" target=\"_blank\">OSM ({r['osm_type']}) {int(r['id'])}</a></b><br>\n",
    "          Etapp: <b>{html.escape(str(r.get('label') or ''))}</b> (√ñ: {html.escape(str(r.get('island') or ''))})<br>\n",
    "          Avst√•nd: ~{r['distance_m']} m<br>\n",
    "          Antal toaletter: <b>{int(r['toilets_num'])}</b>\n",
    "          {img_html}\n",
    "          {kv_html}\n",
    "          {commons_html}\n",
    "          {mapillary_html}\n",
    "          {miss_html}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        Marker(\n",
    "            location=[r[\"lat\"], r[\"lon\"]],\n",
    "            popup=Popup(popup_html, max_width=320),\n",
    "            icon=Icon(color=marker_color, icon=\"info-sign\")\n",
    "        ).add_to(feature_group)\n",
    "\n",
    "# ‚â§200 m lager\n",
    "toilets_pic_fg_0_200 = FeatureGroup(name=\"Toaletter ‚â§200 m (bild, Commons, Mapillary, saknade taggar)\")\n",
    "add_markers(basic_df_0_200, toilets_pic_fg_0_200, marker_color=\"darkblue\")\n",
    "toilets_pic_fg_0_200.add_to(m)\n",
    "\n",
    "# 200‚Äì400 m lager\n",
    "toilets_pic_fg_200_400 = FeatureGroup(name=\"Toaletter 200‚Äì400 m (bild, Commons, Mapillary, saknade taggar)\")\n",
    "add_markers(basic_df_200_400, toilets_pic_fg_200_400, marker_color=\"orange\")\n",
    "toilets_pic_fg_200_400.add_to(m)\n",
    "\n",
    "LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "# =========================\n",
    "# 12) Spara karta + output\n",
    "# =========================\n",
    "map_html = f\"../kartor/Issue_132_toaletter_nara_stockholm_archipelago_trail_{timestamp}.html\"\n",
    "m.save(map_html)\n",
    "\n",
    "print(\"‚úÖ Klart!\")\n",
    "print(f\"‚Ä¢ Summary CSV (‚â§200 m): {summary_csv}\")\n",
    "print(f\"‚Ä¢ Toilets GeoJSON ‚â§200 m: {toilets_geojson_0_200}\")\n",
    "print(f\"‚Ä¢ Toilets CSV ‚â§200 m: {toilets_csv_0_200}\")\n",
    "print(f\"‚Ä¢ Toilets GeoJSON 200‚Äì400 m: {toilets_geojson_200_400}\")\n",
    "print(f\"‚Ä¢ Toilets CSV 200‚Äì400 m: {toilets_csv_200_400}\")\n",
    "print(f\"‚Ä¢ Basic CSV ‚â§200 m: {basic_csv_0_200}\")\n",
    "print(f\"‚Ä¢ Basic CSV 200‚Äì400 m: {basic_csv_200_400}\")\n",
    "print(f\"‚Ä¢ Karta: {map_html}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4961e3a-53b4-4169-a567-e38be677b433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2025-08-20 14:42:12\n",
      "Total time elapsed: 13.57 seconds\n"
     ]
    }
   ],
   "source": [
    " # End timer and calculate duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print current date and total time\n",
    "print(\"Date:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e43025-5c59-4cb8-be5b-6cbe7910eb62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
